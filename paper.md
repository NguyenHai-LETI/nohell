# Разработка нового подхода к алгоритму выбора признаков для задачи классификации многомерных данных
Ключевые слова: алгоритм выбора признаков, оптимизация роя частиц, предварительная обработка данных.
## Аннотация
В этом исследовании сравниваются пять часто используемых алгоритмов выбора признаков, таких как SelectKBest, Recursive Feature Elimination (RFE), Lasso Regression, Bridge Regression и Random Forest, и оценили их преимущества и недостатки. Результат сравнения предоставили критерии для разработки нового алгоритма выбора признаков. Представлена разработка алгоритма, основанного на решении таблиц покрытия (SCT), который нацелен на улучшение классификации высокомерных данных. Чтобы дальше улучшить производительность, алгоритма SCT объединился с алгоритмом оптимизации роек частиц (SCT-PSO). Технически алгоритм SCT-PSO имеет перспективы для выбора признаков в задачах классификации высокомерных данных. Однако практическая эффективность и действенность предложенного алгоритма еще должны быть подтверждены дальнейшими исследованиями с конкретными данными и моделями.
## Введение
С резким ростом технологий количество генерируемых данных увеличивается в значительной мере. Это также благоприятно для области науки о данных. Однако большой и сложный характер данных также представляет собой существенные испытания при их обработке, поэтому важность предварительной обработки данных становится все более выраженной, особенно в отношении алгоритмов выбора признаков. Выбор подходящего алгоритма является неотъемлемым аспектом построения эффективной модели.
Целью данной работы является разработка нового алгоритма выбора признаков, который улучшает недостатки традиционных алгоритмов для моделей классификации с данными высокой размерности.
Объектом исследования являются алгоритмы алгоритмов выбора признаков. Предметом исследования являются Подготовка данных для модели классификации с использованием высокомерных данных.
Для достижения цели были поставлены следующие задачи:
1. Изучить существующие алгоритмы выбора признаков.
2. Определить метод сравнения алгоритмов выбора признаков.
3. Описать новые алгоритмы, которые могут преодолеть недостатки традиционных алгоритмов

## Обзор предметной области
### Принцип отбора аналогов
Для создания метода выбора признаков необходимо провести обзор существующих алгоритмов выбора признаков. Для поиска алгоритмов выявления признаков использовался следующий запрос: ‘traditional feature selection algorithms’
### Аналоги
### SelectKBest feature selection method [1]
SelectKBest выбирает предварительно заданное количество k наиболее эффективных функций на основе указанной функции оценки. Функция оценки может быть любым статистическим тестом, который присваивает оценку каждой функции, такой как хи-квадрат[2], взаимная информация или F-значение ANOVA [3]. Затем SelectKBest выбирает k объектов с наивысшими оценками. Этот метод прост и эффективен и обычно используется в случаях, когда количество признаков велико, и цель состоит в том, чтобы уменьшить пространство признаков, сохранив при этом наиболее информативные признаки. Недостатком алгоритма является то, что выбор оценочной функции является субъективным, и результаты могут быть чувствительны к этому выбору.
### Recursive Feature Elimination (RFE) [4,5]
Алгоритм RFE работает путем рекурсивного удаления наименее важных признаков из набора данных. Алгоритм начинается с полного набора признаков и обучает модель на этом наборе. Значения важности признаков вычисляются на основе коэффициентов модели. Затем удаляется наименее важный признак и процесс повторяется, пока не будет достигнуто желаемое количество признаков. Основными преимуществами алгоритма RFE является его простота, которая делает его легко реализуемым и понятным. Кроме того, показано, что он эффективен при уменьшении количества признаков в наборе данных, что повышает интерпретируемость модели и ее обобщающую способность. Однако есть также и недостатки. Один из главных недостатков - зависимость от выбора используемой модели, что может быть трудно определить на практике.
### Lasso Regression (Least Absolute Shrinkage and Selection Operator) [4]
Lasso Regression - линейной регрессии, который добавляет термин регуляризации в функцию потерь, чтобы стимулировать модель иметь разреженные коэффициенты. Другими словами, Lasso регрессия пытается уменьшить коэффициенты менее важных признаков до нуля, эффективно уменьшая количество используемых в модели признаков. Это делает Lasso Regression полезным инструментом для выбора признаков, а также для предотвращения
переобучения, уменьшая сложность модели. Одним из его преимуществ является способность выполнять выбор признаков, уменьшая коэффициенты менее важных признаков до нуля, эффективно исключая их из модели. Это может улучшить интерпретируемость модели и предотвратить переобучение. Более того, Lasso Regression способен работать с большим количеством признаков, что делает его подходящим для высокомерных наборов данных. Однако может давать смещенные оценки, когда в наборе данных есть сильно коррелированные признаки.
### Ridge Regression [5]
Ridge Regression - линейной регрессии, который добавляет член регуляризации в функцию стоимости, чтобы предотвратить переобучение. В отличие от Lasso Regression[4], которая может создавать разреженные модели, зануляя коэффициенты менее важных признаков, Ridge Regression сужает коэффициенты к нулю, но никогда не устанавливает их равными нулю. Это делает его подходящей для случаев, когда важно сохранять все признаки в модели. Преимущество Ridge Regression является то, что может лучше обрабатывать коллинеарность (высокая корреляция между признаками) по сравнению с обычной линейной регрессией. Однако одним из егог недостатков является то, что может быть сложно выбрать подходящее значение для константы регуляризации, поскольку большее значение приведет к большему уменьшению коэффициентов, а меньшее значение приведет к меньшему уменьшению. Кроме того, гребенчатая регрессия может работать плохо, если в наборе данных имеется большое количество нерелевантных функций.
### Tree-based on feature selection (Random forest) [6]
Random forest — популярный алгоритм машинного обучения на основе дерева, который можно использовать для выбора признаков. Построены несколько деревьев решений с использованием случайных подмножеств функций, а затем агрегирования важности функций, полученных из каждого дерева. Важности признаков может быть использована для ранжирования признаков, а верхние-k признаки могут быть выбраны для использования в модели.
## Критерии сравнения
Были выбраны следующие основные параметры сравнения:
1. Точность: способность выбрать наиболее релевантные признаки, которые вносят вклад в задачу прогнозирования. Это может быть измерено с помощью оценки производительности модели, построенной с использованием выбранных признаков, по сравнению с производительностью модели, построенной с использованием всех признаков.
2. Временная сложность: относится к времени, необходимому алгоритму для завершения
процесса выбора признаков. Это важный фактор, который следует учитывать при работе
с большими наборами данных, так как алгоритмы выбора признаков могут быть
расчетно трудоемкими и занимать много времени.
3. Масштабируемость: способность алгоритма обрабатывать большие наборы данных с
высоким количеством признаков. Это важно, поскольку количество признаков в наборе
данных может существенно влиять на производительность алгоритма и его способность
завершить процесс выбора признаков в приемлемые сроки.
4. Устойчивость: способность алгоритма производить стабильные и согласованные
результаты на различных наборах данных и задачах. Это важно, поскольку алгоритмы
выбора признаков иногда могут производить различные результаты для различных
наборов данных, даже когда наборы данных похожи. Устойчивый алгоритм будет
производить согласованные результаты на широком диапазоне наборов данных.
5. Интерпретируемость: интерпретируемость относится к легкости, с которой результаты
алгоритма можно понять и интерпретировать пользователем. Это важно, поскольку
подходящий алгоритм выбора признаков не только должен производить точные
результаты, но также должен предоставлять ясное и интерпретируемое объяснение
того, как выбраны функции и почему они важны для задачи предсказания.
В табл.1 представлено сравнение алгоритмов выбора признаков.

**Таблица 1 - Сравнительный анализ алгоритмов по количественным критериям**
| Алгоритм выбора признаков       | Точность   | Временная сложность      | Интерпретируемость    | Масштабируемость   | Устойчивость    |
| --------------------------------| -----------| ------------------------ | ----------------------| -------------------| ----------------|
| SelecKBest                      | Хорошо     | Низко                    | Высоко                | Высоко             | Умеренно        | 
| RFE                             | Хорошо     | Высоко                   | Высоко                | Высоко             | Высоко          | 
| Lasso Regression                | Хорошо     | Умеренно                 | Высоко                | Высоко             | Высоко          | 
| Ridge Regression                | Хорошо     | Высоко                   | Высоко                | Высоко             | Высоко          |
| Random Forest                   | Отлично    | Высоко                   | Низко                 | Высоко             | Отлично         |

## Выводы по итогам сравнения
SelectKBest прост и удобен в реализации, в то время как другие, такие как Random Forest, более
сложны, но предлагают потенциал для более высокой производительности. Методы
регуляризации, такие как Lasso и Bridge regression, можно использовать для предотвращения
переобучения и улучшения интерпретируемости модели. Рекурсивное устранение признаков
(RFE) может быть эффективным для уменьшения количества признаков в наборе данных, но
его производительность зависит от выбора используемой модели. Лучший алгоритм выбора
признаков для конкретной задачи будет зависеть от множества факторов, включая размер и сложность набора данных, желаемый результат и доступные вычислительные ресурсы.

## Выбор метода решения
На основании обзора аналогов было установлено, что задачи разработки алгоритма выбора признаков должны удовлетворять следующим требованиям:
1. Алгоритм должен обладать высокой точностью при работе с многомерными наборами данных для задач классификации данных. Эвристический метод будет использоваться в сочетании с алгоритмом выбора признаков для повышения точности.
2. Алгоритмы должны иметь возможность масштабирования для обработки больших и сложных наборов данных.
3. Вычислительная сложность требуется только на среднем уровне, поскольку она зависит от таких факторов, как размер пространства признаков и сложность фитнес-функции.

## Описание метода решения
Для удовлетворения требований, предъявляемых к алгоритму, основанному на решении таблиц покрытия в сочетании с эвристическим методом, именно здесь выбрана оптимизация роя частиц.

### Метод роя частиц (Particle swarm optimization) [9]
Particle Swarm Optimization (PSO) — эвристический алгоритм оптимизации, основанный на социальном поведении стаи птиц или стаи рыб. Был представлен Кеннеди и Эберхартом как метод вычислительного интеллекта для решения задач оптимизации.
В PSO множество возможных решений, представленных в виде частиц, перемещаются в пространстве поиска и корректируют свои позиции на основе собственного опыта и опыта своих соседей. Каждая частица имеет положение и скорость, и скорость каждой частицы обновляется на каждой итерации на основе текущего наилучшего положения частицы и текущего наилучшего положения роя в целом. Затем положение каждой частицы обновляется в зависимости от ее скорости. Этот процесс продолжается до тех пор, пока не будет найдено удовлетворительное решение или не будет выполнен критерий остановки.
В алгоритме PSO участвуют два основных уравнения[10]:
Первое — уравнение скорости (1), где каждая частица в рое обновляет свою скорость, используя вычисленные значения индивидуальных и глобальных лучших решений и свое текущее положение. Коэффициенты 𝑐1 и 𝑐2 являются факторами ускорения, связанными с индивидуальными и социальными аспектами. Известны как параметры доверия: 𝑐1 моделирует степень уверенности частицы в себе, а 𝑐2 моделирует степень уверенности частицы в своих соседях. Вместе со случайными числами 𝑟1 и 𝑟2 определяют стохастический эффект когнитивного и социального поведения:
$$ 𝑣_𝑖^𝑡+1=𝑣_𝑖^𝑡+𝑐_1𝑟_1\left(𝑝𝑏𝑒𝑠𝑡𝑖𝑡−𝑝𝑖𝑡\right)+𝑐_2𝑟_2\left(𝑔𝑏𝑒𝑠𝑡^𝑡−𝑝_i^𝑡\right) $$ (1)
Второе — уравнение положения (2), в котором каждая частица обновляет свое положение, используя вновь вычисленную скорость:
$$ 𝑝_𝑖^𝑡+1=𝑝_𝑖^𝑡+𝑣_𝑖^𝑡+1 $$ (2)
Где,
𝑠𝑖 - особь в рое, 𝑖 ∈[1,𝑛]
𝑝𝑖 - положение частицы 𝑠𝑖
𝑣𝑖 - скорость частицы 𝑝𝑖
𝑝𝑏𝑒𝑠𝑡𝑖 - лучшее решение частицы
𝑔𝑏𝑒𝑠𝑡𝑖 - лучшее решение роя (глобальное)
𝑐1, 𝑐2 - константы ускорения (когнитивные и социальные параметры)
𝑟1, 𝑟2 - случайные числа от 0 до 1
t - номер итерации

## Алгоритм выбора признаков, основанный на решении таблиц покрытия
Алгоритм, основанный на решении таблиц покрытия (SCT) можно использовать в качестве алгоритма выбора признаков для выявления наиболее информативных признаков в многомерном наборе данных. Это особенно полезно в задачах классификации с несколькими классами, где цель состоит в том, чтобы предсказать метку класса выборки на основе большого количества признаков.
Основная идея алгоритма SCT заключается в использовании таблицы покрытия для оценки качества подмножеств признаков, созданных алгоритмом. Таблица покрытия представляет собой матрицу, которая представляет частоту появления каждой функции для каждого класса. Цель алгоритма — найти подмножество признаков, которое максимизирует охват признаков для каждого класса, сохраняя при этом небольшой размер подмножества.
Для достижения этой цели алгоритма SCT использует алгоритм оптимизации, такой как оптимизация роя частиц (PSO), для поиска оптимального подмножества функций. Алгоритм PSO поддерживает множество частиц, представляющих различные решения-кандидаты, и итеративно регулирует положение частиц на основе оценки функции пригодности. Функция пригодности измеряет качество подмножеств признаков, созданных алгоритмом, и предназначена для максимального охвата признаков для каждого класса.

Основные этапы алгоритма SCT-PSO можно резюмировать следующим образом:
1. Предварительная обработка: предварительно обработайте набор данных, чтобы удалить пропущенные значения, выбросы и ненужные функции. При необходимости нормализуйте данные.
2. Построение таблицы покрытия: Создайте таблицу покрытия, чтобы представить частоту появления каждой функции для каждого класса.
3. Инициализация PSO: инициализируйте алгоритм PSO, случайным образом генерируя рой частиц, каждая из которых представляет возможное решение (т. е. подмножество признаков).
4. Итерация PSO: итеративно отрегулируйте положение частиц на основе оценки функции пригодности. Частицы направляются к лучшим решениям как по их собственным наиболее известным позициям, так и по наиболее известным позициям всего роя.
5. Выбор функции: выберите подмножество функций, которое соответствует наиболее известному положению роя, как определено функцией пригодности.
6. Обучение и оценка модели. Обучите модель классификации, используя выбранные функции, и оцените производительность модели, используя соответствующие метрики оценки, такие как точность, достоверность, полнота и оценка F1.

## Оценить алгоритм SCT-PSO согласно заявленным требованиям
1. Точность: Ожидается, что точность алгоритма ASCT-PSO будет выше по сравнению с традиционными алгоритмами выбора признаков по нескольким причинам. Во-первых, ASCT-PSO учитывает охват данных функциями, что может помочь определить более репрезентативный набор функций для задачи классификации. Во-вторых, интеграция ASCT-PSO позволяет более эффективно оптимизировать процесс выбора признаков. Алгоритм PSO может найти глобальное оптимальное решение за относительно короткий промежуток времени, что может привести к повышению точности по сравнению с традиционными алгоритмами выбора признаков, которые используют жадную или исчерпывающую стратегию поиска. Наконец, алгоритм ASCT-PSO учитывает взаимозависимость признаков, что важно для многомерных данных, где корреляции между признаками могут повлиять на точность процесса выбора признаков.
2. Вычислительная сложность: Вычислительная сложность алгоритма ASCT-PSO зависит от нескольких факторов, таких как количество признаков, размер набора данных и конкретная реализация алгоритма. Однако в целом алгоритмы выбора признаков, как правило, имеют доволно высокую вычислительную сложность, поскольку им необходимо оценивать производительность каждого признака.
3. Масштабируемость: Масштабируемость алгоритма ASCT-PSO зависит от нескольких факторов, включая размер набора данных, количество признаков и сложность модели. Как правило, ожидается, что алгоритм ASCT-PSO будет иметь хорошую масштабируемость, когда набор данных не слишком велик, а количество признаков не слишком велико, поскольку он может обрабатывать большое количество измерений.
Кроме того, масштабируемость алгоритма ASCT-PSO можно улучшить за счет оптимизации параметров и алгоритмов, используемых в части алгоритма PSO

## Заключение
В ходе работы был проведён обзор существующих алгоритмов выбора признаков, в ходе которого были выявлены основные требования к требования к новому подходу к алгоритмам выбора признаков. Алгоритм, основанный на решении таблиц покрытия используется в качестве алгоритма выбора признаков. Кроме того, для достижения требований также используется алгоритм PSO для дальнейшего повышения производительности. Также техническая оценка характеристики алгоритма SCT-PSO показывает многообещающий потенциал в решении проблемы классификации данных высокой размерности.
Необходимы дополнительные исследования, чтобы подтвердить эффективность алгоритма ASCT-PSO в реальных моделях и конкретных данных. Тем не менее, наше исследование предлагает новый подход к выбору данных по признакам и лежит в основе будущей работы в этой области.

## Список литературы
1. Sklearn [Электронный ресурс]. URL:
https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html
2. Wikipedia [Электронный ресурс]. URL: https://en.wikipedia.org/wiki/Chi-squared_distribution
3. Medium [Электронный ресурс]. URL:
https://towardsdatascience.com/anova-for-feature-selection-in-machine-learning-d9305e228476
4. Machinelearningmastery [Электронный ресурс]. URL: https://machinelearningmastery.com/rfe-feature-selection-in-python/
5. Columbia Public Health [Электронный ресурс]. URL:
https://www.publichealth.columbia.edu/research/population-health-methods/least-absolute-shrinkage-and-selection-operator-lasso
6. Wikipedia [Электронный ресурс]. URL:
https://en.wikipedia.org/wiki/Lasso_(statistics)
7. Wiley Interdisciplinary Reviews [Электронный ресурс]. URL: https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.14
8. Medium [Электронный ресурс]. URL:
https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f
9. Wikipedia [Электронный ресурс]. URL:
https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D1%80%D0%BE%D1%8F_%D1%87%D0%B0%D1%81%D1%82%D0%B8%D1%86
10. Baeldung [Электронный ресурс]. URL:
https://www.baeldung.com/cs/pso